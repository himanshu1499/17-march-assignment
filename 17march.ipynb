{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5480673-7504-4f7f-a7f8-44903115e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba748789-4a49-4a12-9d3c-38c711004f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upscaling\n",
    "Upsampling involves increasing the number of instances in the minority class\n",
    "\n",
    "# example\n",
    "upsampling may be required is a medical dataset where we are trying to predict the likelihood of a rare disease.\n",
    "If the number of instances with the disease is significantly less than the number of instances without the disease, then the dataset is imbalanced.\n",
    "In this case, we may need to upsample the instances with the disease to balance the class distribution and ensure that the model is trained on enough samples of the rare disease.\n",
    "\n",
    "# downscaling\n",
    "downsampling involves decreasing the number of instances in the majority class.\n",
    "\n",
    "\n",
    "# example\n",
    " downsampling may be required is a fraud detection dataset.\n",
    "    If the number of instances of fraudulent transactions is significantly less than the number of instances of legitimate transactions, then the dataset is imbalanced.\n",
    "    In this case, we may need to downsample the instances of legitimate transactions to balance the class distribution and ensure that the model is trained on enough samples of fraudulent transactions.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b10e4-a4c1-494f-b59f-ef97c8b82798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9da69-8c8f-447a-9447-d3bb15daf8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentaion\n",
    "Data augmentation is a technique used in machine learning and computer vision to increase the size and diversity of a dataset by creating new synthetic examples from the existing ones.\n",
    "It is a common practice when the size of the available dataset is not sufficient for training a robust model. \n",
    "Data augmentation techniques can include flipping, rotating, scaling, cropping, and adding noise to the existing data, among others.\n",
    "\n",
    "\n",
    "#smote\n",
    " A popular data augmentation technique used to address class imbalance problems.\n",
    "    It works by generating new synthetic instances of the minority class by interpolating between existing instances. \n",
    "    The basic idea is to find the k nearest neighbors of each minority class instance and generate new instances along the line segments joining them.\n",
    "    \n",
    "# example\n",
    "let's say we have a dataset with 100 instances, out of which only 10 belong to the minority class.\n",
    "To apply SMOTE, we would first choose a value for k, which determines the number of nearest neighbors to consider.\n",
    "We would then select one of the minority class instances, say instance A, and find its k nearest neighbors from the minority class. \n",
    "SMOTE would then generate a new instance by interpolating between instance A and one of its k nearest neighbors, say instance B, as follows:\n",
    "\n",
    "Select a random value r between 0 and 1.\n",
    "Calculate the difference vector between instance B and instance A, i.e., (B - A).\n",
    "Multiply the difference vector by r, i.e., r * (B - A).\n",
    "Add the resulting vector to instance A, i.e., A + r * (B - A).\n",
    "The resulting vector represents the new synthetic instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9575c-640b-403a-b12e-e40d1a2c5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16ab25-db01-4a18-a429-954055bf38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Outliers are data points that are significantly different from other data points in a dataset.\n",
    "They can be caused by errors in data collection or processing, or they may represent legitimate but rare instances that lie outside the typical range of the data. \n",
    "Outliers can affect statistical analyses and machine learning models by introducing bias and reducing accuracy.\n",
    "\n",
    "It is essential to handle outliers because they can\n",
    "\n",
    "1.Skew the data distribution: Outliers can significantly impact the mean and standard deviation of a dataset, making it difficult to accurately represent the data distribution.\n",
    "\n",
    "2.Impact statistical analyses: Outliers can distort statistical measures such as correlation, regression, and hypothesis testing, leading to incorrect conclusions.\n",
    "\n",
    "3.Reduce model accuracy: Outliers can have a significant impact on machine learning models, leading to poor performance and low accuracy. Models trained on datasets with outliers are likely to overfit to the noise in the data, leading to poor generalization to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bfce8f-2c40-4460-b653-43c93e20f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fab7b4-a3bc-43a6-a3aa-0060ae2ac599",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Complete case analysis: This involves only using cases that have complete data for all variables in the analysis. This approach is simple and easy to implement, but it can lead to a loss of statistical power if there is a large amount of missing data.\n",
    "\n",
    "2.Imputation: Imputation involves replacing missing values with estimated values. There are several types of imputation techniques, including mean imputation, mode imputation, regression imputation, and multiple imputation. Each technique has its advantages and disadvantages, and the choice of technique depends on the nature of the data and the research question.\n",
    "\n",
    "3.Last observation carried forward (LOCF): This technique involves carrying forward the last observed value for a variable for all subsequent missing values. LOCF is commonly used in longitudinal studies where data is collected over time, and missing values are expected.\n",
    "\n",
    "4.Multiple imputation: This technique involves creating multiple imputed datasets by randomly imputing missing values based on the observed data. Multiple imputation produces more accurate estimates of parameters and standard errors and is recommended when missing data is substantial.\n",
    "\n",
    "5.Model-based imputation: This technique involves using a statistical model to estimate missing values. Model-based imputation can provide more accurate estimates of missing values than other imputation techniques but requires a well-specified model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d436fe-fa84-4427-94db-e92227f7b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0983572-2b52-4985-84f4-ea5dd2d49411",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Visual inspection: Plotting the data can help identify patterns in the missing data. Box plots, histograms, and scatterplots can reveal patterns of missingness.\n",
    "\n",
    "2.Descriptive statistics: Calculating descriptive statistics for variables with missing data can help identify patterns of missingness.\n",
    "For example, comparing the mean, median, and variance of variables with missing data to those without missing data can reveal patterns.\n",
    "\n",
    "3.Correlation analysis: Conducting correlation analysis between variables with missing data and those without missing data can reveal patterns of missingness. \n",
    "If the variables with missing data are highly correlated with other variables, it may suggest a non-random pattern of missingness.\n",
    "\n",
    "4.Data mining techniques: Using data mining techniques such as decision trees, clustering, or association rule mining can help identify patterns of missingness.\n",
    "\n",
    "5.Imputation: Imputing missing values using different techniques and comparing the results can help determine if the missing data is MAR or non-random.\n",
    "If the imputed values vary significantly based on the imputation method, it may suggest non-random missingness.\n",
    "\n",
    "6.Statistical tests: Conducting statistical tests such as t-tests or chi-square tests between variables with and without missing data can help determine if the missing data is MAR or non-random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a41df-38c2-4ce3-a080-7adcc09da662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4461a3d9-b6be-4b43-b3f1-2ea2c34fa85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use appropriate performance metrics: Accuracy is not an appropriate metric to evaluate the performance of a model on an imbalanced dataset. Instead, metrics such as precision, recall, F1-score, area under the receiver operating characteristic curve (AUC-ROC), and area under the precision-recall curve (AUC-PR) should be used. These metrics take into account the true positives, true negatives, false positives, and false negatives in the classification problem.\n",
    "\n",
    "Use stratified sampling: When splitting the dataset into training and testing sets, it is important to use stratified sampling to ensure that the distribution of the target variable is maintained in both sets. This can help avoid overfitting and underestimation of performance.\n",
    "\n",
    "Resampling techniques: One approach to handling imbalanced datasets is to use resampling techniques such as oversampling or undersampling. Oversampling involves increasing the number of minority class samples, while undersampling involves reducing the number of majority class samples. However, care should be taken not to oversample the minority class too much, as this can lead to overfitting.\n",
    "\n",
    "Cost-sensitive learning: In some cases, misclassifying the minority class can have more severe consequences than misclassifying the majority class. In such cases, cost-sensitive learning can be used, where the misclassification cost for the minority class is higher than that for the majority class.\n",
    "\n",
    "Ensemble methods: Ensemble methods such as bagging, boosting, and stacking can be used to improve the performance of models on imbalanced datasets by combining multiple models. These methods can help reduce overfitting and improve the generalization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276373d-0c49-43ca-a38e-04b28af9d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00af0f-31b5-4695-9a5d-234c18a0c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random undersampling= This method randomly selects a subset of the majority class to balance the dataset. The main disadvantage of this method is that it may lead to loss of information.\n",
    "\n",
    "Cluster centroids= This method selects the centroids of K-means clusters of the majority class to down-sample the dataset.\n",
    "This method can help reduce the loss of information compared to random undersampling.\n",
    "\n",
    "Tomek links= This method removes samples that are nearest neighbors to samples from the minority class.\n",
    "This method is useful for removing noisy samples from the majority class.\n",
    "\n",
    "Edited nearest neighbor= This method removes samples from the majority class that are misclassified by their nearest neighbors from the minority class.\n",
    "\n",
    "NearMiss= This method selects samples from the majority class that are closest to the minority class samples.\n",
    "It has several variants such as NearMiss-1, NearMiss-2, and NearMiss-3, which differ in how they identify the nearest samples.\n",
    "\n",
    "Combination of oversampling and undersampling= This method involves combining both oversampling and undersampling techniques to balance the dataset.\n",
    "For instance, you can oversample the minority class using techniques like SMOTE and then undersample the majority class to create a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4fd02d-0c45-4dab-a51e-dbc80880457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bacf766-4a2b-493a-afe1-ba3f807b3ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "Missing values are data points that are not recorded or unavailable in a dataset.\n",
    "These missing values can occur due to various reasons such as human errors, system errors, or data corruption. It is essential to handle missing values in a dataset because they can affect the quality of the analysis or model performance. \n",
    "\n",
    "#essential\n",
    "Missing values can bias statistical analysis: If the missing values are not handled properly, they can affect the mean, variance, and other statistical measures of the data, leading to biased results.\n",
    "\n",
    "Missing values can lead to inaccurate model performance: If the missing values are not handled properly, they can lead to inaccurate predictions or classifications, affecting the overall performance of the model.\n",
    "\n",
    "Missing values can lead to reduced sample size: If the missing values are not handled properly, they can lead to a reduced sample size, affecting the statistical power and generalizability of the analysis or model.\n",
    "\n",
    "\n",
    "#algorithms\n",
    "Tree-based algorithms: Decision trees, Random forests, and Gradient boosting are not affected by missing values, as they can handle them internally by branching on the available features.\n",
    "\n",
    "Distance-based algorithms: K-nearest neighbors (KNN) and hierarchical clustering algorithms can handle missing values by ignoring the missing values during the distance calculation.\n",
    "\n",
    "Rule-based algorithms: Rule-based algorithms such as association rule mining and decision rules can handle missing values by assigning default values or treating missing values as a separate category.\n",
    "\n",
    "Bayesian networks: Bayesian networks can handle missing values by treating them as unobserved variables and using probability theory to estimate their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f4d54-0305-4e6a-b605-06c8d9414fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15c929-7010-4748-bf87-19b2de0d2bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imbalanced data\n",
    "Imbalanced data refers to a dataset where the number of examples for each class is not equally distributed. In other words, one class has a significantly larger number of examples than the other(s). For example, in a binary classification problem, if 90% of the samples belong to one class and only 10% to the other class, the dataset is imbalanced.\n",
    "\n",
    "If imbalanced data is not handled, the resulting model can be biased towards the majority class, leading to poor performance for the minority class. In such a case, the model may have high accuracy for the majority class but low recall for the minority class. This means that the model is not able to correctly identify instances of the minority class, which can be critical in applications such as medical diagnosis, fraud detection, or anomaly detection.\n",
    "\n",
    "Moreover, the evaluation metrics such as accuracy can be misleading in imbalanced data because they are biased towards the majority class. Therefore, it is essential to handle imbalanced data to avoid such biases and to ensure that the model performs well for all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369521f1-3492-49e3-aed4-7018f394dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c1702a-c733-4b6d-a57b-772f4a7d950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting missing data\n",
    "One way to handle missing data is to simply delete the rows or columns that contain missing values. This is usually done when the missing data is small in proportion to the total data and is unlikely to affect the overall analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986790df-8485-4888-9307-8291a98bfcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data with missing values\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "\n",
    "# Drop columns with missing values\n",
    "df_dropped_cols = df.dropna(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389604a9-3e32-429b-84fa-5d9a3ea73652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing missing data\n",
    "Another common approach is to impute the missing values with some substitute value. One simple method is to replace missing values with the mean or median of the corresponding feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe5edf0-5be0-439a-a54a-cbe3d8550de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load data with missing values\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
